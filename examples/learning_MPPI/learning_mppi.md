目的： 阐明Learning-MPPI 优于一般的强化学习？？？ 合理吗

Learning-MPPI 降低了 MPPI的计算消耗
你在解决什么问题？ 使用强化学习的方式对物体有更快的反应 ？ 
1. 一般算法无法那么快的响应吗？ 
2. 一般算法的机械臂运动姿态欠佳
   
使用传统算法有无轻易实现的可能?
1. 能直接基于点云进行预测 追踪 -> 给MPPI的响应提供更多的时间
2. 结合全局路径规划，进行规划联控


# 曾经的想法，要进行调研落实
使用强化学习为什么不行？ 

集成MPPI的强化学习算法有很多，但是难度在于对环境的表征：

    对环境的表征难度大，特征提取的难度大，很少有人做
    环境表征难度大
    现阶段的有无通用方法？


不管怎样，要对前一阵子学的强化学习收一个尾巴，无论好坏
以这种心态，给自己一个星期的时间，将learning MPPI予以实现，不基于图片，仅验证mppi + rl 是否可以有效的避免局部最小值

vel 模式在暂时不行，就试试acc先，我们的首要任务是尝试 MPQ 是否可以解决问题

1. 首先要在二维小车上进行验证： 以simple_reacher task展开讲讲
   
15：36 以td-mpc为例
16: 00 SAC在固定初始点|终点的条件下，效果初步成功 ： 
1. 在每一个step下加入-reward以期望收敛步长 
2. 调试exp() 与 步长的惩戒关系让点位正常截止
下一步：
任意始发点 : 
    700 次效果不好，寄希望于多训练几次： 小车左右摆动，尝试翻越障碍区域 
acc模式下
探讨加入tstep的必要




强化学习要放弃，硬件原因 自身能力  实验室没有人做







使用强化学习的方式在速度空间进行设计，这种方式是否合理？ 是否增加了状态可控可观的难度

simple_reacher 例子中， 使用传统的MPPI算法为什么在vel模式下不能实现？ 未解决

现阶段 最主要的一个探索就是learning MPPI ;熟练最基本的一个框架


## 1. TD-MPPI的简单复现
 不管怎样，先把这个写出来，当做对之前一段时间的交代吧
使用td-mppi的简单实现框架 
td-mppi
# 1.1 mppi plan部分
首先这一部分

算法不落地  就是耍流氓 

   
N_policy_traj = N_mppi * radio (0.05) : action = actor(state)



2. SAC算法实践有问题：

训练结果不够好

借鉴spin, 初始阶段使用sample_action增加随机性 | update尝试设置多步

simple_reacher.yml加速设置能否提升呢 : 提升了， 为什么呢？ 速度的提升，扰动系统，提高了探索能力？ 但是路径走的是偏远的，然后在终端无法正常截止，反复横跳
鉴于探索诱因的分析，增加初始阶段sample_action环节，以期能够更好的探索 /. 不行，直接卡在局部最小值出不来了 ———bad 为什么？
提高target_entropy?

总是会有陷入局部最小值的情况，而如果有一条轨迹能够跳出局部最小值，往后的效果往往表现的较好


SAC在生病前为什么表现的较好？ 改了哪个参数了呢？
总的来说 表现并不是很好  | 如果使用learning_mppi 会好一点吗


3. MPPI在vel模式下有问题：
   
看不出具体效果，轨迹变化不明显，路径僵化 时间每有调整对？



learning MPPI 实践

基本上是在acc模式下， state的时候要注意tstep的设计 


想法： 
1. 有没有能预估位置的sdf， mppi在t时刻 rollout那么多条轨迹的时候，t ~ t_H 机械臂的位置状态不同，所用的sdf地图是否也可以不同呢？ 就是我的sdf也是可以在t~t_H时刻预演的
不光只是让 障碍物变大，要合理的障碍物的位置、速度信息去决策可能的行为
如果要做到这一点： 
比如有一段人体预测的轨迹，在t时刻能够得到t~t_H所有的轨迹数据，假设是能得到

2. 